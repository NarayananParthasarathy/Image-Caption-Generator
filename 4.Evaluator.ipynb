{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## AUTHOR : Narayanan Parthasarathy (pnarayanan@rocketmail.com)\n",
    "## Evaluator program written as part of capstone project on  Image Captioning. Program used to calculate BLEU (n-gram), METEOR\n",
    "## ROUGE (n-gram), SPICE metrics. \n",
    "\n",
    "\n",
    "# Input : (1) Actuals / Ground truth. In this case, it will be a text file / object containing all real captions with the image\n",
    "# name as key. This will beb used as reference values for evaluaiton. This will come as separate dataset / file only for those \n",
    "# items in the predictions list(2) Predictions / candidates with 1 generated caption along with the image name as key. This  \n",
    "# will be a separate dataset / file\n",
    "\n",
    "# Output : For each prediction (/ for each image ID), the following scores will be calculated with 'predicted caption' Vs 'all\n",
    "# reference captions' for that image. Finally these individual scores can be averages for the entire dataset.  \n",
    "# Scores to be calculated :\n",
    "# 1. Sentance BLUE - 1-gram, 2-gram, 3-gram, 4-gram [4 items]\n",
    "# 2. METOR - 1 meteor score [1 item]\n",
    "# 3. ROUGE  - rouge-1, rouge-2,rouge-3,rouge-4,rouge-l,rouge-w ; with each containing P,R,F1 scores. AVerage across all \n",
    "#    reference will be taken.\n",
    "# ROUGE-N: Overlap of N-grams between the system and reference summaries\n",
    "# ROUGE-L: Longest Common Subsequence (LCS)based statistics. Longest common subsequence problem takes into account sentence level structure similarity naturally and identifies longest co-occurring in sequence n-grams automatically.\n",
    "# ROUGE-W: Weighted LCS-based statistics that favors consecutive LCSes ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NOTE : This programs works best on a local computer & not ready for cloud yet - mainly due to the fact that the SPICE\n",
    "## evaluatin depnds on standadford-nltk packahge & a local SPICE1.0 jar to be in the local. \n",
    "\n",
    "## Installation needed befreo you proceed in below lines: All commenetd, uncomment them/ run them from your command / shell.\n",
    "# pip install nltk\n",
    "# import nltk\n",
    "# nltk.download() --> This downloads necessary files for nltk. \n",
    "\n",
    "# pip install py-rouge --> For ROUGE score\n",
    "\n",
    "# Once all done, download the below zip file for nltk from http://nlp.stanford.edu/software/stanford-corenlp-full-2015-12-09.zip\n",
    "# and extract the stanford-corenlp-3.6.0.jar & stanford-corenlp-3.6.0-models.jar in the lib folder \n",
    "# without the dependies of SPICE.jar, the spice metrics will not work properly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.stem import WordNetLemmatizer,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "\n",
    "import rouge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import threading\n",
    "import json\n",
    "import ast\n",
    "import tempfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path of the actual captions as well as prediction files. These will be hard coded for now, can be replaced / parameterized. \n",
    "ACTUALS_PATH = 'key_caption_master_final.txt' # path of actual/keyP_caption file. This is a text \n",
    "                                              #file with each line havign a caption in format \"file_name | caption_text\", separated by \\n\n",
    "PREDICTIONS_PATH ='XCEPTION_3GRU256_3E.csv'  # path of the generated caption in csv format. Each image will have 1 caption, with the same foirmat as above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sentence):\n",
    "    sentence=str(sentence)\n",
    "    sentence = sentence.lower()\n",
    "    #sentence=sentence.replace('{html}',\"\") \n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', sentence)\n",
    "    #rem_url=re.sub(r'http\\S+', '',cleantext)\n",
    "    rem_num = re.sub('[0-9]+', '', cleantext)\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokenizer.tokenize(rem_num)  \n",
    "    filtered_words = [w for w in tokens if len(w) > 1 if not w in stopwords.words('english')]\n",
    "    #stem_words=[stemmer.stem(w) for w in filtered_words]\n",
    "    #lemma_words=[lemmatizer.lemmatize(w) for w in stem_words]\n",
    "    return \" \".join(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_eval_scores_1pred(img_name,pred_caption,actual_captions,rouge_ngram_count):      \n",
    "# This function to calcuate the scores for every individul captions (per image), given the candidate caption & ground truth captions\n",
    "# Used for alculating BLEU, METEOR & ROUGE scores. \n",
    "# For ROUGE, the calculations happen for n-grams in this routice but will be ignored for results.  Code is left to run to get the resulyts for analysis, if needed\n",
    "\n",
    "    df_score = pd.DataFrame()\n",
    "    \n",
    "    df_score['image_name'] = [img_name]\n",
    "    df_score['caption_count'] = [len(actual_captions)]\n",
    "    candidate = pred_caption\n",
    "    df_score['predict_caption'] = candidate   ## FOR DEBUG PURPOSE ; TO BE REMOVED LATER\n",
    "    candidate_w_split = candidate[0].split()\n",
    "    reference = []\n",
    "    reference__w_split = []\n",
    "    j=1\n",
    "    for i in actual_captions:\n",
    "        reference.append(i)\n",
    "        df_score[f'actual_{j}'] = i\n",
    "        j=j+1\n",
    "        caption = i.split()\n",
    "        reference__w_split.append(caption)\n",
    "\n",
    "    # SENTENCE BLEU    \n",
    "    df_score['S-BLEU-1']=[sentence_bleu(reference__w_split, candidate_w_split, weights=(1.0, 0, 0, 0))]\n",
    "    df_score['S-BLEU-2']=[sentence_bleu(reference__w_split, candidate_w_split, weights=(0.5, 0.5, 0, 0))]\n",
    "    df_score['S-BLEU-3']=[sentence_bleu(reference__w_split, candidate_w_split, weights=(0.3, 0.3, 0.3, 0))]\n",
    "    df_score['S-BLEU-4']=[sentence_bleu(reference__w_split, candidate_w_split, weights=(0.25, 0.25, 0.25, 0.25))]      \n",
    "  \n",
    "    # METEOR\n",
    "    df_score['METEOR'] = meteor_score(reference, candidate[0])\n",
    "    \n",
    "    #ROUGE , with Apply Average\n",
    "    evaluator = rouge.Rouge(metrics=['rouge-n', 'rouge-l', 'rouge-w'],\n",
    "                           max_n=rouge_ngram_count,\n",
    "                           limit_length=True,\n",
    "                           length_limit=100,\n",
    "                           length_limit_type='words',\n",
    "                           apply_avg=True,\n",
    "                           apply_best=False,\n",
    "                           alpha=0.5, # Default F1_score\n",
    "                           weight_factor=1.2,\n",
    "                           stemming=True)\n",
    "    rouge_scores = evaluator.get_scores(reference[0], candidate[0])\n",
    "    for metric, results in sorted(rouge_scores.items(), key=lambda x: x[0]):        \n",
    "        if metric == 'rouge-1' :\n",
    "            df_score['rouge-1-p'] = results['p']\n",
    "            df_score['rouge-1-r'] = results['r']\n",
    "            df_score['rouge-1-f'] = results['f']\n",
    "        elif metric == 'rouge-2':\n",
    "            df_score['rouge-2-p'] = results['p']\n",
    "            df_score['rouge-2-r'] = results['r']\n",
    "            df_score['rouge-2-f'] = results['f']\n",
    "        elif metric == 'rouge-3':\n",
    "            df_score['rouge-3-p'] = results['p']\n",
    "            df_score['rouge-3-r'] = results['r']\n",
    "            df_score['rouge-3-f'] = results['f']\n",
    "        elif metric == 'rouge-4':\n",
    "            df_score['rouge-4-p'] = results['p']\n",
    "            df_score['rouge-4-r'] = results['r']\n",
    "            df_score['rouge-4-f'] = results['f']\n",
    "        elif metric == 'rouge-l' :\n",
    "            df_score['rouge-l-p'] = results['p']\n",
    "            df_score['rouge-l-r'] = results['r']\n",
    "            df_score['rouge-l-f'] = results['f']\n",
    "        elif metric == 'rouge-w' :\n",
    "            df_score['rouge-w-p'] = results['p']\n",
    "            df_score['rouge-w-r'] = results['r']\n",
    "            df_score['rouge-w-f'] = results['f']\n",
    "\n",
    "    return df_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def float_convert(obj):\n",
    "        try:\n",
    "          return float(obj)\n",
    "        except:\n",
    "          return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spice_score_all_pred(input_data):\n",
    "# Function to calculate spice score. Since this is an call to external java program, to reduce execution time / avoiding multipel calls\n",
    "# this functions takes all candidate captions & their respective ground truths, convert them into a json format\n",
    "# uses SPICE_JAR to calculate the scores & dumps the results in a file with json format. \n",
    "    #SPICE\n",
    "    # Assumes spice.jar is in the same directory as spice.py.  Change as needed.\n",
    "    SPICE_JAR = 'spice-1.0.jar'  # Location of SPICE-1.0.jar\n",
    "    TEMP_DIR = 'tmp'             # location of temp dir\n",
    "    CACHE_DIR = 'cache'          # location of cache dir\n",
    "        \n",
    "    cwd = \"C:\\\\Users\\\\narayanan.p\\\\Documents\\\\01 Machine Learning\\\\Great Learning - AIML\\\\CAPSTONE\\\\CODE\\\\Evaluator\\\\\"  \n",
    "    temp_dir=os.path.join(cwd, TEMP_DIR)\n",
    "    if not os.path.exists(temp_dir):\n",
    "      os.makedirs(temp_dir)\n",
    "    in_file = tempfile.NamedTemporaryFile(delete=False, dir=temp_dir,mode='w')\n",
    "    json.dump(input_data, in_file, indent=2)\n",
    "    in_file.close()\n",
    "    \n",
    "    # Start job\n",
    "    out_file = tempfile.NamedTemporaryFile(delete=False, dir=temp_dir)\n",
    "    out_file.close()\n",
    "    cache_dir=os.path.join(cwd, CACHE_DIR)\n",
    "    \n",
    "    if not os.path.exists(cache_dir):\n",
    "      os.makedirs(cache_dir)\n",
    "    spice_cmd = ['java', '-jar', '-Xmx8G', SPICE_JAR, in_file.name, \n",
    "      '-out', out_file.name,      \n",
    "      '-subset', '-silent'\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        subprocess.check_call(spice_cmd)\n",
    "    except subprocess.CalledProcessError as error:\n",
    "        print(error)\n",
    "       \n",
    "    \n",
    "    # Read and process results\n",
    "    with open(out_file.name) as data_file:    \n",
    "         results = json.load(data_file)\n",
    "    # Uncomment the next two lines if you want the temp files to be deleted. Commented here so that the temp files can be used for analysis\n",
    "    #os.remove(in_file.name)  \n",
    "    #os.remove(out_file.name)\n",
    "    \n",
    "    dfcols =['image_name', 'spice_pr', 'spice_re','spice_f']\n",
    "    df_spice_scores = pd.DataFrame(columns=dfcols)\n",
    "   \n",
    "    for item in results:\n",
    "        df_spice_scores = df_spice_scores.append({'image_name':item['image_id'],'spice_pr':float_convert(item['scores']['All']['pr']),'spice_re':float_convert(item['scores']['All']['re']),'spice_f':float_convert(item['scores']['All']['f']) },ignore_index=True)\n",
    "    \n",
    "    return df_spice_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actuals = pd.read_csv(ACTUALS_PATH,sep='|',header = None)\n",
    "df_predictions =pd.read_csv(PREDICTIONS_PATH,sep='|',header = None)\n",
    "df_actuals.rename(columns={ 0: 'file_name', 1 : 'Caption'},inplace=True)\n",
    "df_predictions.rename(columns={ 0: 'file_name', 1 : 'Caption'},inplace=True)\n",
    "df_actuals.sort_values(by =['file_name'],ascending=True,inplace=True)\n",
    "df_predictions.sort_values(by =['file_name'],ascending=True,inplace=True)\n",
    "\n",
    "# Convert all to lower case, remove unwanted characters\n",
    "df_actuals['Caption'] = df_actuals['Caption'].str.lower()\n",
    "df_predictions['Caption'] = df_predictions['Caption'].str.lower()\n",
    "\n",
    "df_actuals['Caption'] = df_actuals['Caption'].replace('[^a-zA-Z0-9 ]', '', regex=True)\n",
    "df_predictions['Caption'] = df_predictions['Caption'].replace('[^a-zA-Z0-9 ]', '', regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEMP CODE FOR MATCHING PREDICTIONS TO KEY_CAPTIONS. Redundant caode, can be removed\n",
    "df_predictions['file_name'] = df_predictions['file_name'].str.strip()\n",
    "df_actuals['file_name'] = df_actuals['file_name'].str.strip()\n",
    "\n",
    "\n",
    "keys = list(df_predictions.columns.values)\n",
    "keys = keys[0]\n",
    "i1 = df_actuals.set_index(keys).index\n",
    "i2 = df_predictions.set_index(keys).index\n",
    "df_actuals = df_actuals[i1.isin(i2)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_actuals = df_actuals.sort_values(by=['file_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\nltk\\translate\\bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "df_all_scores = pd.DataFrame()\n",
    "rouge_ngram_count = 4 \n",
    "spice_input_data = []\n",
    "for i,j in df_predictions.iterrows():\n",
    "    df_filter = df_actuals.loc[df_actuals['file_name'].str.strip() ==j[0].strip()]\n",
    "    if df_filter.shape[0] != 0 :  # SKIP IF THE GENERATED CAPTION FILE NAME IS INCORRECT / NO MATCHES FOUND IN ACTUALS\n",
    "        img_name = j[0].strip()\n",
    "        pred_caption = []\n",
    "        pred_caption.append(j[1].strip())\n",
    "        actual_caption = []\n",
    "        for a,b in df_filter.iterrows():        \n",
    "            actual_caption.append(b[1].strip())\n",
    "        \n",
    "        #print(actual_caption)\n",
    "        df_score=get_all_eval_scores_1pred(img_name,pred_caption,actual_caption,rouge_ngram_count)\n",
    "        df_all_scores = pd.concat([df_all_scores,df_score],ignore_index=True)\n",
    "\n",
    "        spice_input_data.append({\n",
    "                  \"image_id\" : img_name,\n",
    "                  \"test\" : pred_caption[0],\n",
    "                  \"refs\" : actual_caption\n",
    "                })\n",
    "\n",
    "df_spice_scores = get_spice_score_all_pred(spice_input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE XL WITH SUBSET OF COLUMNS FROM THE DF_ALL_SCORES AS WELL AS CALCULATE THE AVERAGES.\n",
    "df_scores_report = pd.DataFrame()\n",
    "df_scores_report['image_name'] = df_all_scores['image_name']\n",
    "df_scores_report['caption_count'] = df_all_scores['caption_count']\n",
    "df_scores_report['BLEU-1'] = df_all_scores['S-BLEU-1']\n",
    "df_scores_report['BLEU-2'] = df_all_scores['S-BLEU-2']\n",
    "df_scores_report['BLEU-3'] = df_all_scores['S-BLEU-3']\n",
    "df_scores_report['BLEU-4'] = df_all_scores['S-BLEU-4']\n",
    "df_scores_report['METEOR'] = df_all_scores['METEOR']\n",
    "df_scores_report['ROUGE_precision'] = df_all_scores['rouge-l-p']\n",
    "df_scores_report['ROUGE_recall'] = df_all_scores['rouge-l-r']\n",
    "df_scores_report['ROUGE_FScore'] = df_all_scores['rouge-l-f']\n",
    "\n",
    "df_scores_report = pd.merge(df_scores_report,df_spice_scores,on='image_name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output file names basis the input predictiosn files\n",
    "base_name = PREDICTIONS_PATH.split('.')[0]\n",
    "Output_Scores = base_name+ '_SCORES_ALL.csv'\n",
    "Output_Summary = base_name+ '_SCORES_SUMMARY.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scores_report.to_csv(Output_Scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\narayanan.p\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: The signature of `Series.to_csv` was aligned to that of `DataFrame.to_csv`, and argument 'header' will change its default value from False to True: please pass an explicit value to suppress this warning.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "df_scores_summary = df_scores_report.mean(axis=0)\n",
    "df_scores_summary['caption_count'] = df_predictions.shape[0]\n",
    "df_scores_summary.to_csv(Output_Summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DONE\n"
     ]
    }
   ],
   "source": [
    "print(\"DONE\") # Print Done when complete. Added here since the SPICE calculation will take more than 20 mins on a core i3 laptop."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
